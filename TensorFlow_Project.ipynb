{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#git commands in Git Bash to push commits in GitHub\n",
    "##cd Personal/Self_Study/Projects/MyProject\n",
    "##git add TensorFlow_Project.ipynb\n",
    "##git commit -m \"Update TensorFlow_Project.ipynb\"\n",
    "##git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example code to understand how tensorflow works\n",
    "#with made up xy data for regression model\n",
    "\n",
    "\n",
    "#imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "#define the NN\n",
    "##this function builds a models with one hidden layer and just one neuron with one input value\n",
    "model = tf.keras.Sequential([keras.layers.Dense(units=1,input_shape=[1])])\n",
    "#compile the NN\n",
    "##this function tries to learn by \n",
    "##1. starting with a random relation beween the input and output\n",
    "##2. calculating the difference between real and estimated output (loss)\n",
    "##3. changing the relation in order to reduce the loss (optimizer)\n",
    "##4. repeat steps 2 and 3 for the number of epochs\n",
    "model.compile(optimizer='sgd',loss='mean_squared_error')\n",
    "\n",
    "#providing the data\n",
    "##the exact relation is y = (2*x)-1\n",
    "xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
    "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\n",
    "\n",
    "#training the NN\n",
    "model.fit(xs,ys,epochs=500)\n",
    "\n",
    "#try to predict\n",
    "##the exact value is (2*10)-1 = 19\n",
    "print(model.predict(np.array([10.0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example code to understand how tensorflow works\n",
    "#dataset from fashionMNIST (curated by Zalando) using its API in tensorflow\n",
    "#this is classification problem with 28X28 pixel grayscale images of clothes as input and labels as output\n",
    "#each pixel can take any value from 0 to 255\n",
    "\n",
    "#imports\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "#dataset\n",
    "#get the fashionMNIST data from the tensorflow API\n",
    "mnist = tf.keras.datasets.fashion_mnist #create an object to get the data\n",
    "#there are about 70K data entries out of which 60K is automatically allocated for training and 10K is allocated for testing\n",
    "(trainingImages,trainingLabels),(testImages,testLabels) = mnist.load_data() #load train and test data from object\n",
    "#visualizing the dataset\n",
    "import matplotlib.pyplot as plt\n",
    "imgNum = 7\n",
    "plt.imshow(trainingImages[imgNum])\n",
    "print(trainingLabels[imgNum])\n",
    "print(trainingImages[imgNum])\n",
    "#normalizing the data to enable better learning of the model\n",
    "trainingImages = trainingImages/255.0\n",
    "testImages = testImages/255.0\n",
    "\n",
    "#print(len([item for sublist in trainingImages[imgNum] for item in sublist]))\n",
    "\n",
    "\n",
    "#define the NN model\n",
    "##Sequential: That defines a SEQUENCE of layers in the neural network\n",
    "##Flatten: Remember earlier where our images were a square, when you printed them out? Flatten just takes that square and turns it into a 1 dimensional set.\n",
    "##Dense: Adds a layer of neurons\n",
    "##Each layer of neurons need an activation function to tell them what to do. There's lots of options, but just use these for now.\n",
    "##Relu effectively means \"If X>0 return X, else return 0\" -- so what it does is it only passes values 0 or greater to the next layer in the network.\n",
    "##Softmax takes a set of values, and effectively picks the biggest one, so, for example, if the output of the last layer looks like [0.1, 0.1, 0.05, 0.1, 9.5, 0.1, 0.05, 0.05, 0.05], it saves you from fishing through it looking for the biggest value, and turns it into [0,0,0,0,1,0,0,0,0] -- The goal is to save a lot of coding!\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),tf.keras.layers.Dense(128,activation=tf.nn.relu),tf.keras.layers.Dense(10,activation=tf.nn.softmax)])\n",
    "\n",
    "#compile the NN model\n",
    "##what are the different optimizer and loss parameters???\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#train the NN model with the training data\n",
    "model.fit(trainingImages,trainingLabels,epochs=5)\n",
    "\n",
    "#evaluating the model with the test data\n",
    "model.evaluate(testImages,testLabels)\n",
    "\n",
    "\n",
    "# #using callbacks to terminate the program when a desired accuracy is reached\n",
    "# desiredAcc = 0.6\n",
    "# class myCallback(tf.keras.callbacks.Callback):\n",
    "#   def on_epoch_end(self, epoch, logs={}):\n",
    "#     if(logs.get('loss')<(1-desiredAcc)):\n",
    "#       print(\"\\nReached desired accuracy so cancelling training!\")\n",
    "#       self.model.stop_training = True\n",
    "\n",
    "# callbacks = myCallback()\n",
    "\n",
    "# model.fit(training_images, training_labels, epochs=5, callbacks=[callbacks])\n",
    "\n",
    "\n",
    "\n",
    "#parameters that can change\n",
    "##training test dataset distribution\n",
    "##NN architecture - number of hidden layers and neurons in each layer\n",
    "##normalization of data\n",
    "##number of iterations (epochs)\n",
    "##optimizer used\n",
    "##loss function used\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using callbacks to terminate the program when a desired accuracy is reached\n",
    "\n",
    "desiredAcc = 0.7 #1 is 100% accuracy\n",
    "\n",
    "#create a class for the callback which will take the log and check for a certain condition\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "#its good to use epoch_end since sometimes loss drops over a small data range but not on the whole data\n",
    "  def on_epoch_end(self, epoch, logs={}): \n",
    "    if(logs.get('loss')<(1-desiredAcc)):\n",
    "      print(\"\\nReached desired accuracy so cancelling training!\")\n",
    "      self.model.stop_training = True\n",
    "\n",
    "#instantiating the callback class\n",
    "callbacks = myCallback()\n",
    "\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(trainingImages,trainingLabels),(testImages,testLabels) = mnist.load_data()\n",
    "trainingImages = trainingImages/255.0\n",
    "testImages = testImages/255.0\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),tf.keras.layers.Dense(128,activation=tf.nn.relu),tf.keras.layers.Dense(10,activation=tf.nn.softmax)])\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit(trainingImages, trainingLabels, epochs=10, callbacks=[callbacks]) # use callback as a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the Flatten argument can be used with exact shape of the input\n",
    "#the accuracy can be selected instead of the loss value in the callback function\n",
    "#using callbacks to terminate the program when a desired accuracy is reached\n",
    "\n",
    "desiredAcc = 0.9 #1 is 100% accuracy\n",
    "\n",
    "#create a class for the callback which will take the log and check for a certain condition\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "#its good to use epoch_end since sometimes loss drops over a small data range but not on the whole data\n",
    "  def on_epoch_end(self, epoch, logs={}): \n",
    "    if(logs.get('acc')>desiredAcc):\n",
    "      print(\"\\nReached desired accuracy so cancelling training!\")\n",
    "      self.model.stop_training = True\n",
    "\n",
    "#instantiating the callback class\n",
    "callbacks = myCallback()\n",
    "\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(trainingImages,trainingLabels),(testImages,testLabels) = mnist.load_data()\n",
    "trainingImages = trainingImages/255.0\n",
    "testImages = testImages/255.0\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28,28)),tf.keras.layers.Dense(128,activation=tf.nn.relu),tf.keras.layers.Dense(10,activation=tf.nn.softmax)])\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit(trainingImages, trainingLabels, epochs=10, callbacks=[callbacks]) # use callback as a parameter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
